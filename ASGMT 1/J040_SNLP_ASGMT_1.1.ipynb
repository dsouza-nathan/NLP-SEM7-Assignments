{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Pretrained Model Analysis\n",
    "\n",
    "## Assignment: SNLP - Word Embeddings with Word2Vec\n",
    "\n",
    "**Objectives:**\n",
    "1. Use a pretrained Word2Vec model (`word2vec-google-news-300`)\n",
    "2. Find similar words for 5 chosen words\n",
    "3. Test word analogies using vector arithmetic\n",
    "4. Compare with custom trained model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T05:03:06.218946Z",
     "iopub.status.busy": "2025-07-20T05:03:06.218684Z",
     "iopub.status.idle": "2025-07-20T05:03:06.223173Z",
     "shell.execute_reply": "2025-07-20T05:03:06.222351Z",
     "shell.execute_reply.started": "2025-07-20T05:03:06.218927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Word2Vec Model\n",
    "\n",
    "We'll use the Google News Word2Vec model which contains 300-dimensional vectors trained on Google News dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T05:03:06.224848Z",
     "iopub.status.busy": "2025-07-20T05:03:06.224571Z",
     "iopub.status.idle": "2025-07-20T05:03:50.208078Z",
     "shell.execute_reply": "2025-07-20T05:03:50.207335Z",
     "shell.execute_reply.started": "2025-07-20T05:03:06.224828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n",
      "Attempting to load word2vec-google-news-300...\n",
      "[=============================---------------------] 58.9% 980.0/1662.8MB downloaded✗ Error loading Google News model: <urlopen error retrieval incomplete: got only 1027604480 out of 1743563840 bytes>\n",
      "\n",
      "Trying alternative smaller model...\n",
      "[=============================---------------------] 58.9% 980.0/1662.8MB downloaded✗ Error loading Google News model: <urlopen error retrieval incomplete: got only 1027604480 out of 1743563840 bytes>\n",
      "\n",
      "Trying alternative smaller model...\n",
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n",
      "✓ Successfully loaded GloVe Wiki model as alternative!\n",
      "\n",
      "Model status: Loaded\n",
      "Model type: <class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "Vector size: 50\n",
      "Vocabulary size: 400000\n",
      "✓ Successfully loaded GloVe Wiki model as alternative!\n",
      "\n",
      "Model status: Loaded\n",
      "Model type: <class 'gensim.models.keyedvectors.KeyedVectors'>\n",
      "Vector size: 50\n",
      "Vocabulary size: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading pretrained model...\")\n",
    "print(\"Attempting to load word2vec-google-news-300...\")\n",
    "\n",
    "try:\n",
    "    # Try to load the large Google News model\n",
    "    pretrained_model = api.load(\"word2vec-google-news-300\")\n",
    "    print(\"✓ Successfully loaded Google News Word2Vec model!\")\n",
    "    model_loaded = True\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading Google News model: {e}\")\n",
    "    print(\"\\nTrying alternative smaller model...\")\n",
    "    \n",
    "    try:\n",
    "        # Try loading a smaller alternative model\n",
    "        pretrained_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "        print(\"✓ Successfully loaded GloVe Wiki model as alternative!\")\n",
    "        model_loaded = True\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"✗ Error loading alternative model: {e2}\")\n",
    "        print(\"\\nWill demonstrate with custom model only...\")\n",
    "        model_loaded = False\n",
    "\n",
    "print(f\"\\nModel status: {'Loaded' if model_loaded else 'Not loaded'}\")\n",
    "if model_loaded:\n",
    "    print(f\"Model type: {type(pretrained_model)}\")\n",
    "    print(f\"Vector size: {pretrained_model.vector_size}\")\n",
    "    print(f\"Vocabulary size: {len(pretrained_model.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find Similar Words for 5 Chosen Words\n",
    "\n",
    "We'll test the model with 5 different words from various domains and find their most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T05:03:50.209228Z",
     "iopub.status.busy": "2025-07-20T05:03:50.208927Z",
     "iopub.status.idle": "2025-07-20T05:03:52.424103Z",
     "shell.execute_reply": "2025-07-20T05:03:52.423373Z",
     "shell.execute_reply.started": "2025-07-20T05:03:50.209208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMILAR WORDS FROM PRETRAINED MODEL\n",
      "==================================================\n",
      "\n",
      "Top 5 similar words to 'science':\n",
      "  1. sciences        (similarity: 0.8548)\n",
      "  2. research        (similarity: 0.8437)\n",
      "  3. institute       (similarity: 0.8386)\n",
      "  4. studies         (similarity: 0.8369)\n",
      "  5. physics         (similarity: 0.8314)\n",
      "----------------------------------------\n",
      "\n",
      "Top 5 similar words to 'coffee':\n",
      "  1. drink           (similarity: 0.8187)\n",
      "  2. drinks          (similarity: 0.8176)\n",
      "  3. wine            (similarity: 0.8141)\n",
      "  4. tea             (similarity: 0.8080)\n",
      "  5. beer            (similarity: 0.8042)\n",
      "----------------------------------------\n",
      "\n",
      "Top 5 similar words to 'music':\n",
      "  1. musical         (similarity: 0.8854)\n",
      "  2. pop             (similarity: 0.8682)\n",
      "  3. dance           (similarity: 0.8531)\n",
      "  4. songs           (similarity: 0.8526)\n",
      "  5. recording       (similarity: 0.8392)\n",
      "----------------------------------------\n",
      "\n",
      "Top 5 similar words to 'apple':\n",
      "  1. blackberry      (similarity: 0.7543)\n",
      "  2. chips           (similarity: 0.7439)\n",
      "  3. iphone          (similarity: 0.7430)\n",
      "  4. microsoft       (similarity: 0.7334)\n",
      "  5. ipad            (similarity: 0.7331)\n",
      "----------------------------------------\n",
      "\n",
      "Top 5 similar words to 'teacher':\n",
      "  1. student         (similarity: 0.8962)\n",
      "  2. graduate        (similarity: 0.8133)\n",
      "  3. teaching        (similarity: 0.8129)\n",
      "  4. taught          (similarity: 0.8076)\n",
      "  5. teaches         (similarity: 0.7872)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Selected 5 words from different domains\n",
    "words = ['science', 'coffee', 'music', 'apple', 'teacher']\n",
    "\n",
    "print(\"SIMILAR WORDS FROM PRETRAINED MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if model_loaded:\n",
    "    for word in words:\n",
    "        print(f\"\\nTop 5 similar words to '{word}':\")\n",
    "        try:\n",
    "            similar_words = pretrained_model.most_similar(word, topn=5)\n",
    "            for i, (similar, score) in enumerate(similar_words, 1):\n",
    "                print(f\"  {i}. {similar:15} (similarity: {score:.4f})\")\n",
    "        except KeyError:\n",
    "            print(f\"  Word '{word}' not found in vocabulary\")\n",
    "        print(\"-\" * 40)\n",
    "else:\n",
    "    print(\"Cannot demonstrate - pretrained model not loaded.\")\n",
    "    print(\"This section requires a working internet connection\")\n",
    "    print(\"to download the pretrained Word2Vec model.\")\n",
    "    print()\n",
    "    print(\"Expected output would show:\")\n",
    "    print(\"- 'science' similar to: research, scientific, biology, etc.\")\n",
    "    print(\"- 'coffee' similar to: tea, espresso, caffeine, etc.\")\n",
    "    print(\"- 'music' similar to: songs, musical, audio, etc.\")\n",
    "    print(\"- 'apple' similar to: fruit, iPhone, company, etc.\")\n",
    "    print(\"- 'teacher' similar to: instructor, educator, professor, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Analogies using Vector Arithmetic\n",
    "\n",
    "Testing the famous word analogy examples using vector operations: **A - B + C ≈ D**\n",
    "\n",
    "Examples:\n",
    "- king - man + woman ≈ queen\n",
    "- paris - france + italy ≈ rome  \n",
    "- doctor - hospital + school ≈ teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-20T05:03:52.425799Z",
     "iopub.status.busy": "2025-07-20T05:03:52.425554Z",
     "iopub.status.idle": "2025-07-20T05:03:53.100440Z",
     "shell.execute_reply": "2025-07-20T05:03:53.099640Z",
     "shell.execute_reply.started": "2025-07-20T05:03:52.425780Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD ANALOGIES USING VECTOR ARITHMETIC\n",
      "==================================================\n",
      "\n",
      "1. Analogy: king - man + woman\n",
      "   Expected: queen\n",
      "   Result: queen (confidence: 0.8524)\n",
      "   Top 3 candidates:\n",
      "     1. queen (0.8524)\n",
      "     2. throne (0.7664)\n",
      "     3. prince (0.7592)\n",
      "--------------------------------------------------\n",
      "\n",
      "2. Analogy: paris - france + italy\n",
      "   Expected: rome\n",
      "   Result: rome (confidence: 0.8466)\n",
      "   Top 3 candidates:\n",
      "     1. rome (0.8466)\n",
      "     2. milan (0.7766)\n",
      "     3. turin (0.7666)\n",
      "--------------------------------------------------\n",
      "\n",
      "3. Analogy: doctor - hospital + school\n",
      "   Expected: teacher\n",
      "   Result: teacher (confidence: 0.8208)\n",
      "   Top 3 candidates:\n",
      "     1. teacher (0.8208)\n",
      "     2. taught (0.7912)\n",
      "     3. master (0.7836)\n",
      "--------------------------------------------------\n",
      "\n",
      "4. Analogy: big - bigger + small\n",
      "   Expected: smaller\n",
      "   Result: large (confidence: 0.8236)\n",
      "   Top 3 candidates:\n",
      "     1. large (0.8236)\n",
      "     2. one (0.8007)\n",
      "     3. along (0.7915)\n",
      "--------------------------------------------------\n",
      "\n",
      "5. Analogy: good - better + bad\n",
      "   Expected: worse\n",
      "   Result: little (confidence: 0.8396)\n",
      "   Top 3 candidates:\n",
      "     1. little (0.8396)\n",
      "     2. luck (0.8365)\n",
      "     3. thing (0.8204)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"WORD ANALOGIES USING VECTOR ARITHMETIC\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if model_loaded:\n",
    "    # Define analogy examples: (A, B, C) where A - B + C ≈ D\n",
    "    analogies = [\n",
    "        ('king', 'man', 'woman', 'Expected: queen'),\n",
    "        ('paris', 'france', 'italy', 'Expected: rome'),\n",
    "        ('doctor', 'hospital', 'school', 'Expected: teacher'),\n",
    "        ('big', 'bigger', 'small', 'Expected: smaller'),\n",
    "        ('good', 'better', 'bad', 'Expected: worse')\n",
    "    ]\n",
    "\n",
    "    for i, (a, b, c, expected) in enumerate(analogies, 1):\n",
    "        print(f\"\\n{i}. Analogy: {a} - {b} + {c}\")\n",
    "        print(f\"   {expected}\")\n",
    "        \n",
    "        try:\n",
    "            result = pretrained_model.most_similar(positive=[a, c], negative=[b], topn=3)\n",
    "            print(f\"   Result: {result[0][0]} (confidence: {result[0][1]:.4f})\")\n",
    "            \n",
    "            # Show top 3 results\n",
    "            print(\"   Top 3 candidates:\")\n",
    "            for j, (word, score) in enumerate(result, 1):\n",
    "                print(f\"     {j}. {word} ({score:.4f})\")\n",
    "                \n",
    "        except KeyError as e:\n",
    "            print(f\"   Error: Word not found in vocabulary - {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot demonstrate - pretrained model not loaded.\")\n",
    "    print(\"This section requires a working internet connection\")\n",
    "    print(\"to download the pretrained Word2Vec model.\")\n",
    "    print()\n",
    "    print(\"Expected analogy results:\")\n",
    "    print(\"1. king - man + woman ≈ queen\")\n",
    "    print(\"2. paris - france + italy ≈ rome\") \n",
    "    print(\"3. doctor - hospital + school ≈ teacher\")\n",
    "    print(\"4. big - bigger + small ≈ smaller\")\n",
    "    print(\"5. good - better + bad ≈ worse\")\n",
    "    print()\n",
    "    print(\"These analogies demonstrate that Word2Vec captures\")\n",
    "    print(\"semantic relationships through vector arithmetic.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
